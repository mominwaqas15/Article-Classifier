{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Test Size: 0.2 -----\n",
      "\n",
      "\n",
      "Optimal Results:\n",
      "Optimal Accuracy: 0.81\n",
      "Optimal Parameters: {'ngram_range': 2, 'alpha': 0.1}\n",
      "\n",
      "----- Test Size: 0.15 -----\n",
      "\n",
      "\n",
      "Optimal Results:\n",
      "Optimal Accuracy: 0.81\n",
      "Optimal Parameters: {'ngram_range': 2, 'alpha': 0.1}\n",
      "\n",
      "----- Test Size: 0.1 -----\n",
      "\n",
      "\n",
      "Optimal Results:\n",
      "Optimal Accuracy: 0.78\n",
      "Optimal Parameters: {'ngram_range': 2, 'alpha': 0.5}\n",
      "\n",
      "----- Test Size: 0.05 -----\n",
      "\n",
      "\n",
      "Optimal Results:\n",
      "Optimal Accuracy: 0.84\n",
      "Optimal Parameters: {'ngram_range': 2, 'alpha': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Convert frozenset to a list\n",
    "custom_stopwords = list(ENGLISH_STOP_WORDS)\n",
    "\n",
    "df = pd.read_csv('subset.csv')\n",
    "df.dropna(subset=['Combined_Content', 'Label'], inplace=True)\n",
    "\n",
    "# 'Combined_Content' column and 'Label' column\n",
    "X = df['Combined_Content']\n",
    "y = df['Label']\n",
    "\n",
    "# Define the range of n-grams and alpha values \n",
    "ngram_range_values = range(1, 6)\n",
    "alpha_values = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "# Iterate over different test sizes\n",
    "for test_size in [0.20, 0.15, 0.10, 0.05]:\n",
    "    print(f\"\\n----- Test Size: {test_size} -----\\n\")\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(X, y, test_size=test_size, random_state=45)\n",
    "\n",
    "    # Initialize variables to store optimal results for each test size\n",
    "    optimal_accuracy = 0.0\n",
    "    optimal_params = None\n",
    "\n",
    "    for n in ngram_range_values:\n",
    "        for alpha in alpha_values:\n",
    "            # Vectorize the text data using unigrams and bigrams (n-grams) with stopwords removal\n",
    "            vectorizer = CountVectorizer(ngram_range=(1, n), stop_words=custom_stopwords)\n",
    "            X_train = vectorizer.fit_transform(train_data)\n",
    "            X_test = vectorizer.transform(test_data)\n",
    "\n",
    "            # Initialize and train the Naive Bayes model\n",
    "            naive_bayes = MultinomialNB(alpha=alpha)\n",
    "            naive_bayes.fit(X_train, train_labels)\n",
    "\n",
    "            # Make predictions on the test set\n",
    "            predictions = naive_bayes.predict(X_test)\n",
    "\n",
    "            # Evaluate the model\n",
    "            accuracy = accuracy_score(test_labels, predictions)\n",
    "            conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "            class_report = classification_report(test_labels, predictions)\n",
    "\n",
    "            # Store the evaluation metrics\n",
    "            if accuracy > optimal_accuracy:\n",
    "                optimal_accuracy = accuracy\n",
    "                optimal_params = {'ngram_range': n, 'alpha': alpha}\n",
    "\n",
    "    # Print the optimal results for each test size\n",
    "    print(\"\\nOptimal Results:\")\n",
    "    print(f\"Optimal Accuracy: {optimal_accuracy:.2f}\")\n",
    "    print(f\"Optimal Parameters: {optimal_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Test Size: 0.2 -----\n",
      "\n",
      "\n",
      "Optimal Results:\n",
      "Optimal Accuracy: 0.91\n",
      "Optimal Parameters: {'ngram_range': 1, 'C_value': 0.5}\n",
      "\n",
      "----- Test Size: 0.15 -----\n",
      "\n",
      "\n",
      "Optimal Results:\n",
      "Optimal Accuracy: 0.89\n",
      "Optimal Parameters: {'ngram_range': 1, 'C_value': 0.5}\n",
      "\n",
      "----- Test Size: 0.1 -----\n",
      "\n",
      "\n",
      "Optimal Results:\n",
      "Optimal Accuracy: 0.88\n",
      "Optimal Parameters: {'ngram_range': 2, 'C_value': 0.1}\n",
      "\n",
      "----- Test Size: 0.05 -----\n",
      "\n",
      "\n",
      "Optimal Results:\n",
      "Optimal Accuracy: 0.88\n",
      "Optimal Parameters: {'ngram_range': 1, 'C_value': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Convert frozenset to a list\n",
    "custom_stopwords = list(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# Load data \n",
    "df = pd.read_csv('subset.csv')\n",
    "df.dropna(subset=['Combined_Content', 'Label'], inplace=True)\n",
    "\n",
    "# 'Combined_Content' column and 'Label' column\n",
    "X = df['Combined_Content']\n",
    "y = df['Label']\n",
    "\n",
    "# Define the range of n-grams and C values \n",
    "ngram_range_values = range(1, 6)\n",
    "C_values = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "# Iterate over different test sizes\n",
    "for test_size in [0.20, 0.15, 0.10, 0.05]:\n",
    "    print(f\"\\n----- Test Size: {test_size} -----\\n\")\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(X, y, test_size=test_size, random_state=45)\n",
    "\n",
    "    # Initialize variables to store optimal results for each test size\n",
    "    optimal_accuracy = 0.0\n",
    "    optimal_params = None\n",
    "\n",
    "    for n in ngram_range_values:\n",
    "        for C_value in C_values:\n",
    "            # Vectorize the text data using unigrams and bigrams (n-grams) with stopwords removal\n",
    "            vectorizer = CountVectorizer(ngram_range=(1, n), stop_words=custom_stopwords)\n",
    "            X_train = vectorizer.fit_transform(train_data)\n",
    "            X_test = vectorizer.transform(test_data)\n",
    "\n",
    "            # Initialize and train the Logistic Regression model\n",
    "            logistic_regression = LogisticRegression(C=C_value, max_iter=1000)\n",
    "            logistic_regression.fit(X_train, train_labels)\n",
    "\n",
    "            # Make predictions on the test set\n",
    "            predictions = logistic_regression.predict(X_test)\n",
    "\n",
    "            # Evaluate the model\n",
    "            accuracy = accuracy_score(test_labels, predictions)\n",
    "            conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "            class_report = classification_report(test_labels, predictions)\n",
    "\n",
    "            # Store the evaluation metrics\n",
    "            if accuracy > optimal_accuracy:\n",
    "                optimal_accuracy = accuracy\n",
    "                optimal_params = {'ngram_range': n, 'C_value': C_value}\n",
    "\n",
    "    # Print the optimal results for each test size\n",
    "    print(\"\\nOptimal Results:\")\n",
    "    print(f\"Optimal Accuracy: {optimal_accuracy:.2f}\")\n",
    "    print(f\"Optimal Parameters: {optimal_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "15/15 [==============================] - 154s 10s/step - loss: -0.9554 - accuracy: 0.2067 - val_loss: -5.6171 - val_accuracy: 0.1200\n",
      "Epoch 2/5\n",
      "15/15 [==============================] - 280s 19s/step - loss: -7.4540 - accuracy: 0.2089 - val_loss: -10.6484 - val_accuracy: 0.1200\n",
      "Epoch 3/5\n",
      "15/15 [==============================] - 299s 20s/step - loss: -11.4607 - accuracy: 0.2089 - val_loss: -13.8588 - val_accuracy: 0.1200\n",
      "Epoch 4/5\n",
      "15/15 [==============================] - 309s 20s/step - loss: -13.9170 - accuracy: 0.2089 - val_loss: -16.0433 - val_accuracy: 0.1200\n",
      "Epoch 5/5\n",
      "15/15 [==============================] - 311s 21s/step - loss: -15.7797 - accuracy: 0.2089 - val_loss: -17.8315 - val_accuracy: 0.1200\n",
      "2/2 [==============================] - 2s 749ms/step - loss: -17.8315 - accuracy: 0.1200\n",
      "Test Accuracy: 12.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "df = pd.read_csv('subset.csv')\n",
    "df.dropna(subset=['Combined_Content', 'Label'], inplace=True)\n",
    "\n",
    "X = df['Combined_Content']\n",
    "y = df['Label']\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(X, y_encoded, test_size=0.10, random_state=45)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(train_data)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = max(len(seq) for seq in X_train_sequences)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=max_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, train_labels, validation_data=(X_test_padded, test_labels), epochs=5, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test_padded, test_labels)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\momin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 12s 594ms/step - loss: 0.0000e+00 - accuracy: 0.2075 - val_loss: 0.0000e+00 - val_accuracy: 0.1700\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 7s 531ms/step - loss: 0.0000e+00 - accuracy: 0.2075 - val_loss: 0.0000e+00 - val_accuracy: 0.1700\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 8s 625ms/step - loss: 0.0000e+00 - accuracy: 0.2075 - val_loss: 0.0000e+00 - val_accuracy: 0.1700\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 9s 702ms/step - loss: 0.0000e+00 - accuracy: 0.2075 - val_loss: 0.0000e+00 - val_accuracy: 0.1700\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 9s 671ms/step - loss: 0.0000e+00 - accuracy: 0.2075 - val_loss: 0.0000e+00 - val_accuracy: 0.1700\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 9s 687ms/step - loss: 0.0000e+00 - accuracy: 0.2075 - val_loss: 0.0000e+00 - val_accuracy: 0.1700\n",
      "4/4 [==============================] - 1s 185ms/step - loss: 0.0000e+00 - accuracy: 0.1700\n",
      "Test Accuracy: 17.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load your data (replace 'your_data.csv' with your actual file)\n",
    "df = pd.read_csv('subset.csv')\n",
    "df = df.dropna(subset=['Combined_Content', 'Label'])\n",
    "\n",
    "X = df['Combined_Content']\n",
    "y = df['Label']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
    "\n",
    "# Tokenize the text data\n",
    "max_words = 5000\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_length = 100\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 500, input_length=max_length))\n",
    "model.add(LSTM(256, dropout=0.15, return_sequences=True))\n",
    "model.add(LSTM(128, dropout=0.15))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model.fit(X_train_padded, y_train, validation_data=(X_test_padded, y_test), epochs=50, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN (Vanilla flavour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "123/123 [==============================] - 11s 68ms/step - loss: 1.8458 - accuracy: 0.1951 - val_loss: 1.6382 - val_accuracy: 0.2175\n",
      "Epoch 2/100\n",
      "123/123 [==============================] - 8s 68ms/step - loss: 1.8432 - accuracy: 0.2002 - val_loss: 1.6247 - val_accuracy: 0.2134\n",
      "Epoch 3/100\n",
      "123/123 [==============================] - 10s 80ms/step - loss: 1.7823 - accuracy: 0.2099 - val_loss: 1.6232 - val_accuracy: 0.2114\n",
      "Epoch 4/100\n",
      "123/123 [==============================] - 10s 79ms/step - loss: 1.7648 - accuracy: 0.2114 - val_loss: 1.6199 - val_accuracy: 0.2398\n",
      "Epoch 5/100\n",
      "123/123 [==============================] - 11s 86ms/step - loss: 1.7689 - accuracy: 0.2022 - val_loss: 1.6231 - val_accuracy: 0.2154\n",
      "Epoch 6/100\n",
      "123/123 [==============================] - 13s 108ms/step - loss: 1.7751 - accuracy: 0.1966 - val_loss: 1.6214 - val_accuracy: 0.2033\n",
      "Epoch 7/100\n",
      "123/123 [==============================] - 12s 101ms/step - loss: 1.7554 - accuracy: 0.2134 - val_loss: 1.6182 - val_accuracy: 0.2012\n",
      "Epoch 8/100\n",
      "123/123 [==============================] - 11s 91ms/step - loss: 1.7543 - accuracy: 0.2027 - val_loss: 1.6088 - val_accuracy: 0.2033\n",
      "Epoch 9/100\n",
      "123/123 [==============================] - 11s 88ms/step - loss: 1.7448 - accuracy: 0.1966 - val_loss: 1.6091 - val_accuracy: 0.1992\n",
      "Epoch 10/100\n",
      "123/123 [==============================] - 11s 87ms/step - loss: 1.7291 - accuracy: 0.2073 - val_loss: 1.6149 - val_accuracy: 0.2033\n",
      "Epoch 11/100\n",
      "123/123 [==============================] - 11s 87ms/step - loss: 1.7165 - accuracy: 0.1941 - val_loss: 1.6205 - val_accuracy: 0.1931\n",
      "Epoch 12/100\n",
      "123/123 [==============================] - 11s 86ms/step - loss: 1.7162 - accuracy: 0.2043 - val_loss: 1.6216 - val_accuracy: 0.1748\n",
      "Epoch 13/100\n",
      "123/123 [==============================] - 11s 86ms/step - loss: 1.6677 - accuracy: 0.2104 - val_loss: 1.6127 - val_accuracy: 0.1768\n",
      "Epoch 14/100\n",
      "123/123 [==============================] - 11s 89ms/step - loss: 1.6564 - accuracy: 0.2053 - val_loss: 1.6103 - val_accuracy: 0.1911\n",
      "Epoch 15/100\n",
      "123/123 [==============================] - 10s 83ms/step - loss: 1.6392 - accuracy: 0.1875 - val_loss: 1.6112 - val_accuracy: 0.1931\n",
      "Epoch 16/100\n",
      "123/123 [==============================] - 11s 87ms/step - loss: 1.6232 - accuracy: 0.2292 - val_loss: 1.6107 - val_accuracy: 0.2073\n",
      "Epoch 17/100\n",
      "123/123 [==============================] - 10s 85ms/step - loss: 1.6193 - accuracy: 0.2134 - val_loss: 1.6100 - val_accuracy: 0.2012\n",
      "Epoch 18/100\n",
      "123/123 [==============================] - 10s 84ms/step - loss: 1.6150 - accuracy: 0.2007 - val_loss: 1.6097 - val_accuracy: 0.1992\n",
      "Epoch 19/100\n",
      "123/123 [==============================] - 10s 84ms/step - loss: 1.6166 - accuracy: 0.2195 - val_loss: 1.6096 - val_accuracy: 0.1992\n",
      "Epoch 20/100\n",
      "123/123 [==============================] - 10s 83ms/step - loss: 1.6133 - accuracy: 0.2170 - val_loss: 1.6095 - val_accuracy: 0.1992\n",
      "Epoch 21/100\n",
      "123/123 [==============================] - 10s 81ms/step - loss: 1.6163 - accuracy: 0.2033 - val_loss: 1.6097 - val_accuracy: 0.1992\n",
      "Epoch 22/100\n",
      "123/123 [==============================] - 10s 80ms/step - loss: 1.6136 - accuracy: 0.2154 - val_loss: 1.6094 - val_accuracy: 0.2195\n",
      "Epoch 23/100\n",
      "123/123 [==============================] - 10s 79ms/step - loss: 1.6132 - accuracy: 0.2073 - val_loss: 1.6094 - val_accuracy: 0.1992\n",
      "16/16 [==============================] - 1s 30ms/step - loss: 1.6088 - accuracy: 0.2033\n",
      "Test Accuracy: 20.33%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load your data (replace 'your_data.csv' with your actual file)\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Assuming your data has 'Combined_Content' column and 'Label' column\n",
    "# Concatenate 'Title' and 'Article Content' columns\n",
    "df['Combined_Content'] = df['Title'] + ' ' + df['Article Content']\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.dropna(subset=['Combined_Content', 'Label'])\n",
    "\n",
    "X = df['Combined_Content']\n",
    "y = df['Label']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
    "\n",
    "# Tokenize the text data\n",
    "max_words = 5000\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_length = 100\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 200, input_length=max_length))\n",
    "model.add(SimpleRNN(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(SimpleRNN(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "model.fit(X_train_padded, y_train, validation_data=(X_test_padded, y_test), epochs=100, batch_size=16, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### increased data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "123/123 [==============================] - 75s 577ms/step - loss: 1.6094 - accuracy: 0.2002 - val_loss: 1.6091 - val_accuracy: 0.2317\n",
      "Epoch 2/150\n",
      "123/123 [==============================] - 88s 714ms/step - loss: 1.6086 - accuracy: 0.2226 - val_loss: 1.6086 - val_accuracy: 0.2398\n",
      "Epoch 3/150\n",
      "123/123 [==============================] - 87s 709ms/step - loss: 1.6078 - accuracy: 0.2464 - val_loss: 1.6082 - val_accuracy: 0.2256\n",
      "Epoch 4/150\n",
      "123/123 [==============================] - 84s 682ms/step - loss: 1.6075 - accuracy: 0.2520 - val_loss: 1.6079 - val_accuracy: 0.2378\n",
      "Epoch 5/150\n",
      "123/123 [==============================] - 85s 695ms/step - loss: 1.6063 - accuracy: 0.2591 - val_loss: 1.6074 - val_accuracy: 0.2480\n",
      "Epoch 6/150\n",
      "123/123 [==============================] - 78s 637ms/step - loss: 1.6049 - accuracy: 0.2576 - val_loss: 1.6066 - val_accuracy: 0.2520\n",
      "Epoch 7/150\n",
      "123/123 [==============================] - 89s 723ms/step - loss: 1.6048 - accuracy: 0.2586 - val_loss: 1.6056 - val_accuracy: 0.2642\n",
      "Epoch 8/150\n",
      "123/123 [==============================] - 89s 724ms/step - loss: 1.6030 - accuracy: 0.2846 - val_loss: 1.6041 - val_accuracy: 0.2724\n",
      "Epoch 9/150\n",
      "123/123 [==============================] - 95s 776ms/step - loss: 1.5999 - accuracy: 0.2912 - val_loss: 1.6017 - val_accuracy: 0.2866\n",
      "Epoch 10/150\n",
      "123/123 [==============================] - 91s 742ms/step - loss: 1.5971 - accuracy: 0.3262 - val_loss: 1.5995 - val_accuracy: 0.2927\n",
      "Epoch 11/150\n",
      "123/123 [==============================] - 92s 748ms/step - loss: 1.5931 - accuracy: 0.3445 - val_loss: 1.5967 - val_accuracy: 0.3008\n",
      "Epoch 12/150\n",
      "123/123 [==============================] - 88s 719ms/step - loss: 1.5703 - accuracy: 0.3598 - val_loss: 1.4516 - val_accuracy: 0.3272\n",
      "Epoch 13/150\n",
      "123/123 [==============================] - 74s 599ms/step - loss: 1.3617 - accuracy: 0.3745 - val_loss: 1.3181 - val_accuracy: 0.3720\n",
      "Epoch 14/150\n",
      "123/123 [==============================] - 88s 716ms/step - loss: 1.3076 - accuracy: 0.3963 - val_loss: 1.3044 - val_accuracy: 0.3679\n",
      "Epoch 15/150\n",
      "123/123 [==============================] - 82s 669ms/step - loss: 1.2996 - accuracy: 0.3821 - val_loss: 1.3096 - val_accuracy: 0.3740\n",
      "Epoch 16/150\n",
      "123/123 [==============================] - 82s 666ms/step - loss: 1.2974 - accuracy: 0.3974 - val_loss: 1.2886 - val_accuracy: 0.3638\n",
      "Epoch 17/150\n",
      "123/123 [==============================] - 81s 661ms/step - loss: 1.2913 - accuracy: 0.3928 - val_loss: 1.2895 - val_accuracy: 0.3638\n",
      "Epoch 18/150\n",
      "123/123 [==============================] - 78s 630ms/step - loss: 1.2784 - accuracy: 0.4106 - val_loss: 1.2977 - val_accuracy: 0.3638\n",
      "Epoch 19/150\n",
      "123/123 [==============================] - 80s 650ms/step - loss: 1.2835 - accuracy: 0.3770 - val_loss: 1.2896 - val_accuracy: 0.3659\n",
      "Epoch 20/150\n",
      "123/123 [==============================] - 78s 633ms/step - loss: 1.2740 - accuracy: 0.3857 - val_loss: 1.2975 - val_accuracy: 0.3659\n",
      "Epoch 21/150\n",
      "123/123 [==============================] - 80s 654ms/step - loss: 1.2725 - accuracy: 0.3938 - val_loss: 1.2966 - val_accuracy: 0.3618\n",
      "Epoch 22/150\n",
      "123/123 [==============================] - 82s 669ms/step - loss: 1.2722 - accuracy: 0.3984 - val_loss: 1.2939 - val_accuracy: 0.3679\n",
      "Epoch 23/150\n",
      "123/123 [==============================] - 83s 673ms/step - loss: 1.2618 - accuracy: 0.3872 - val_loss: 1.2974 - val_accuracy: 0.3679\n",
      "Epoch 24/150\n",
      "123/123 [==============================] - 81s 660ms/step - loss: 1.2519 - accuracy: 0.3968 - val_loss: 1.2913 - val_accuracy: 0.3659\n",
      "Epoch 25/150\n",
      "123/123 [==============================] - 76s 614ms/step - loss: 1.2444 - accuracy: 0.3958 - val_loss: 1.2835 - val_accuracy: 0.3598\n",
      "Epoch 26/150\n",
      "123/123 [==============================] - 73s 593ms/step - loss: 1.2599 - accuracy: 0.3821 - val_loss: 1.2841 - val_accuracy: 0.3699\n",
      "Epoch 27/150\n",
      "123/123 [==============================] - 73s 594ms/step - loss: 1.2450 - accuracy: 0.4045 - val_loss: 1.2810 - val_accuracy: 0.3618\n",
      "Epoch 28/150\n",
      "123/123 [==============================] - 93s 756ms/step - loss: 1.2422 - accuracy: 0.4040 - val_loss: 1.2806 - val_accuracy: 0.3679\n",
      "Epoch 29/150\n",
      "123/123 [==============================] - 86s 702ms/step - loss: 1.2418 - accuracy: 0.3968 - val_loss: 1.2722 - val_accuracy: 0.3720\n",
      "Epoch 30/150\n",
      "123/123 [==============================] - 80s 649ms/step - loss: 1.2351 - accuracy: 0.4040 - val_loss: 1.2643 - val_accuracy: 0.3801\n",
      "Epoch 31/150\n",
      "123/123 [==============================] - 84s 685ms/step - loss: 1.2175 - accuracy: 0.4202 - val_loss: 1.2631 - val_accuracy: 0.3801\n",
      "Epoch 32/150\n",
      "123/123 [==============================] - 85s 692ms/step - loss: 1.2164 - accuracy: 0.4385 - val_loss: 1.2673 - val_accuracy: 0.4045\n",
      "Epoch 33/150\n",
      "123/123 [==============================] - 72s 584ms/step - loss: 1.1906 - accuracy: 0.4599 - val_loss: 1.2504 - val_accuracy: 0.4329\n",
      "Epoch 34/150\n",
      "123/123 [==============================] - 69s 564ms/step - loss: 1.1832 - accuracy: 0.4553 - val_loss: 1.2320 - val_accuracy: 0.4634\n",
      "Epoch 35/150\n",
      "123/123 [==============================] - 71s 577ms/step - loss: 1.1589 - accuracy: 0.4751 - val_loss: 1.2244 - val_accuracy: 0.4715\n",
      "Epoch 36/150\n",
      "123/123 [==============================] - 73s 598ms/step - loss: 1.1509 - accuracy: 0.4949 - val_loss: 1.2071 - val_accuracy: 0.4797\n",
      "Epoch 37/150\n",
      "123/123 [==============================] - 70s 572ms/step - loss: 1.1325 - accuracy: 0.5127 - val_loss: 1.1984 - val_accuracy: 0.4817\n",
      "Epoch 38/150\n",
      "123/123 [==============================] - 70s 566ms/step - loss: 1.1278 - accuracy: 0.4975 - val_loss: 1.1877 - val_accuracy: 0.4858\n",
      "Epoch 39/150\n",
      "123/123 [==============================] - 68s 555ms/step - loss: 1.1232 - accuracy: 0.5102 - val_loss: 1.1882 - val_accuracy: 0.4756\n",
      "Epoch 40/150\n",
      "123/123 [==============================] - 70s 572ms/step - loss: 1.1057 - accuracy: 0.5107 - val_loss: 1.1930 - val_accuracy: 0.4695\n",
      "Epoch 41/150\n",
      "123/123 [==============================] - 70s 571ms/step - loss: 1.1040 - accuracy: 0.5386 - val_loss: 1.1660 - val_accuracy: 0.5020\n",
      "Epoch 42/150\n",
      "123/123 [==============================] - 75s 612ms/step - loss: 1.0804 - accuracy: 0.5376 - val_loss: 1.1594 - val_accuracy: 0.5000\n",
      "Epoch 43/150\n",
      "123/123 [==============================] - 70s 570ms/step - loss: 1.0513 - accuracy: 0.5437 - val_loss: 1.1588 - val_accuracy: 0.4959\n",
      "Epoch 44/150\n",
      "123/123 [==============================] - 66s 537ms/step - loss: 1.0659 - accuracy: 0.5427 - val_loss: 1.1552 - val_accuracy: 0.4959\n",
      "Epoch 45/150\n",
      "123/123 [==============================] - 68s 552ms/step - loss: 1.0496 - accuracy: 0.5442 - val_loss: 1.1920 - val_accuracy: 0.4756\n",
      "Epoch 46/150\n",
      "123/123 [==============================] - 85s 692ms/step - loss: 1.0342 - accuracy: 0.5569 - val_loss: 1.1617 - val_accuracy: 0.4959\n",
      "Epoch 47/150\n",
      "123/123 [==============================] - 91s 735ms/step - loss: 1.0317 - accuracy: 0.5666 - val_loss: 1.1595 - val_accuracy: 0.4980\n",
      "Epoch 48/150\n",
      "123/123 [==============================] - 77s 625ms/step - loss: 1.0113 - accuracy: 0.5676 - val_loss: 1.1567 - val_accuracy: 0.4776\n",
      "Epoch 49/150\n",
      "123/123 [==============================] - 86s 697ms/step - loss: 1.0049 - accuracy: 0.5772 - val_loss: 1.1638 - val_accuracy: 0.4756\n",
      "Epoch 50/150\n",
      "123/123 [==============================] - 83s 678ms/step - loss: 1.0171 - accuracy: 0.5635 - val_loss: 1.1725 - val_accuracy: 0.4736\n",
      "Epoch 51/150\n",
      "123/123 [==============================] - 87s 707ms/step - loss: 1.0032 - accuracy: 0.5788 - val_loss: 1.1748 - val_accuracy: 0.4654\n",
      "Epoch 52/150\n",
      "123/123 [==============================] - 86s 702ms/step - loss: 0.9964 - accuracy: 0.5783 - val_loss: 1.1639 - val_accuracy: 0.5020\n",
      "Epoch 53/150\n",
      "123/123 [==============================] - 80s 653ms/step - loss: 0.9848 - accuracy: 0.5803 - val_loss: 1.1747 - val_accuracy: 0.4756\n",
      "Epoch 54/150\n",
      "123/123 [==============================] - 82s 663ms/step - loss: 0.9594 - accuracy: 0.6042 - val_loss: 1.1831 - val_accuracy: 0.4817\n",
      "Epoch 55/150\n",
      "123/123 [==============================] - 91s 743ms/step - loss: 0.9467 - accuracy: 0.5935 - val_loss: 1.1760 - val_accuracy: 0.4919\n",
      "Epoch 56/150\n",
      "123/123 [==============================] - 88s 712ms/step - loss: 0.9550 - accuracy: 0.6062 - val_loss: 1.1761 - val_accuracy: 0.4715\n",
      "Epoch 57/150\n",
      "123/123 [==============================] - 92s 747ms/step - loss: 0.9666 - accuracy: 0.5986 - val_loss: 1.1771 - val_accuracy: 0.4817\n",
      "Epoch 58/150\n",
      "123/123 [==============================] - 94s 764ms/step - loss: 0.9585 - accuracy: 0.6143 - val_loss: 1.1767 - val_accuracy: 0.4817\n",
      "Epoch 59/150\n",
      "123/123 [==============================] - 92s 748ms/step - loss: 0.9545 - accuracy: 0.6016 - val_loss: 1.2069 - val_accuracy: 0.4695\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 1.1552 - accuracy: 0.4959\n",
      "Test Accuracy: 49.59%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "df['Combined_Content'] = df['Title'] + ' ' + df['Article Content']\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.dropna(subset=['Combined_Content', 'Label'])\n",
    "\n",
    "X = df['Combined_Content']\n",
    "y = df['Label']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
    "\n",
    "# Tokenize the text data\n",
    "max_words = 5000\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_length = 100\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 200, input_length=max_length))\n",
    "model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.00001)  # Adjusted learning rate\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)  # Increased patience\n",
    "model.fit(X_train_padded, y_train, validation_data=(X_test_padded, y_test), epochs=150, batch_size=16, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN (increased layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "62/62 [==============================] - 934s 15s/step - loss: 1.6090 - accuracy: 0.2160 - val_loss: 1.6088 - val_accuracy: 0.2012\n",
      "Epoch 2/150\n",
      "62/62 [==============================] - 1195s 19s/step - loss: 1.6079 - accuracy: 0.2241 - val_loss: 1.6077 - val_accuracy: 0.2134\n",
      "Epoch 3/150\n",
      "62/62 [==============================] - 1226s 20s/step - loss: 1.6056 - accuracy: 0.2373 - val_loss: 1.6053 - val_accuracy: 0.2297\n",
      "Epoch 4/150\n",
      "62/62 [==============================] - 1123s 18s/step - loss: 1.6013 - accuracy: 0.2424 - val_loss: 1.6000 - val_accuracy: 0.2256\n",
      "Epoch 5/150\n",
      "62/62 [==============================] - 1054s 17s/step - loss: 1.5913 - accuracy: 0.2637 - val_loss: 1.5496 - val_accuracy: 0.2886\n",
      "Epoch 6/150\n",
      "62/62 [==============================] - 1083s 17s/step - loss: 1.4040 - accuracy: 0.3572 - val_loss: 1.2561 - val_accuracy: 0.3984\n",
      "Epoch 7/150\n",
      "62/62 [==============================] - 1087s 18s/step - loss: 1.2926 - accuracy: 0.3760 - val_loss: 1.2542 - val_accuracy: 0.3679\n",
      "Epoch 8/150\n",
      "62/62 [==============================] - 1262s 20s/step - loss: 1.2708 - accuracy: 0.3887 - val_loss: 1.2448 - val_accuracy: 0.3862\n",
      "Epoch 9/150\n",
      "62/62 [==============================] - 1212s 20s/step - loss: 1.2605 - accuracy: 0.3963 - val_loss: 1.2347 - val_accuracy: 0.4146\n",
      "Epoch 10/150\n",
      "62/62 [==============================] - 1248s 20s/step - loss: 1.2540 - accuracy: 0.3902 - val_loss: 1.2353 - val_accuracy: 0.3963\n",
      "Epoch 11/150\n",
      "62/62 [==============================] - 1291s 21s/step - loss: 1.2430 - accuracy: 0.4045 - val_loss: 1.2437 - val_accuracy: 0.3780\n",
      "Epoch 12/150\n",
      "62/62 [==============================] - 1284s 21s/step - loss: 1.2485 - accuracy: 0.3974 - val_loss: 1.2338 - val_accuracy: 0.4126\n",
      "Epoch 13/150\n",
      "62/62 [==============================] - 1267s 20s/step - loss: 1.2383 - accuracy: 0.4009 - val_loss: 1.2310 - val_accuracy: 0.3760\n",
      "Epoch 14/150\n",
      "62/62 [==============================] - 1370s 22s/step - loss: 1.2145 - accuracy: 0.4253 - val_loss: 1.2243 - val_accuracy: 0.3923\n",
      "Epoch 15/150\n",
      "62/62 [==============================] - 1283s 21s/step - loss: 1.2151 - accuracy: 0.4131 - val_loss: 1.1938 - val_accuracy: 0.4715\n",
      "Epoch 16/150\n",
      "62/62 [==============================] - 1256s 20s/step - loss: 1.2037 - accuracy: 0.4502 - val_loss: 1.1789 - val_accuracy: 0.4411\n",
      "Epoch 17/150\n",
      "62/62 [==============================] - 1247s 20s/step - loss: 1.1611 - accuracy: 0.4624 - val_loss: 1.1526 - val_accuracy: 0.5224\n",
      "Epoch 18/150\n",
      "62/62 [==============================] - 1264s 20s/step - loss: 1.1451 - accuracy: 0.4903 - val_loss: 1.1266 - val_accuracy: 0.5203\n",
      "Epoch 19/150\n",
      "62/62 [==============================] - 1264s 20s/step - loss: 1.1460 - accuracy: 0.4695 - val_loss: 1.1762 - val_accuracy: 0.4736\n",
      "Epoch 20/150\n",
      "62/62 [==============================] - 1248s 20s/step - loss: 1.1264 - accuracy: 0.5036 - val_loss: 1.1219 - val_accuracy: 0.4878\n",
      "Epoch 21/150\n",
      "62/62 [==============================] - 1262s 20s/step - loss: 1.1043 - accuracy: 0.5005 - val_loss: 1.1155 - val_accuracy: 0.5020\n",
      "Epoch 22/150\n",
      "62/62 [==============================] - 1249s 20s/step - loss: 1.1151 - accuracy: 0.4909 - val_loss: 1.1123 - val_accuracy: 0.5183\n",
      "Epoch 23/150\n",
      "62/62 [==============================] - 1254s 20s/step - loss: 1.0985 - accuracy: 0.5071 - val_loss: 1.1293 - val_accuracy: 0.4959\n",
      "Epoch 24/150\n",
      "62/62 [==============================] - 1248s 20s/step - loss: 1.0893 - accuracy: 0.4959 - val_loss: 1.1036 - val_accuracy: 0.5224\n",
      "Epoch 25/150\n",
      "62/62 [==============================] - 1287s 21s/step - loss: 1.0650 - accuracy: 0.5158 - val_loss: 1.0942 - val_accuracy: 0.5203\n",
      "Epoch 26/150\n",
      "62/62 [==============================] - 1271s 20s/step - loss: 1.0824 - accuracy: 0.5030 - val_loss: 1.0745 - val_accuracy: 0.5264\n",
      "Epoch 27/150\n",
      "62/62 [==============================] - 1276s 21s/step - loss: 1.0616 - accuracy: 0.5213 - val_loss: 1.1058 - val_accuracy: 0.5102\n",
      "Epoch 28/150\n",
      "62/62 [==============================] - 1276s 21s/step - loss: 1.0655 - accuracy: 0.5178 - val_loss: 1.0720 - val_accuracy: 0.5447\n",
      "Epoch 29/150\n",
      "62/62 [==============================] - 1282s 21s/step - loss: 1.0626 - accuracy: 0.5249 - val_loss: 1.0733 - val_accuracy: 0.5244\n",
      "Epoch 30/150\n",
      "62/62 [==============================] - 1280s 21s/step - loss: 1.0469 - accuracy: 0.5340 - val_loss: 1.1264 - val_accuracy: 0.5000\n",
      "Epoch 31/150\n",
      "62/62 [==============================] - 1287s 21s/step - loss: 1.0640 - accuracy: 0.5249 - val_loss: 1.0651 - val_accuracy: 0.5285\n",
      "Epoch 32/150\n",
      "62/62 [==============================] - 1293s 21s/step - loss: 1.0360 - accuracy: 0.5335 - val_loss: 1.0695 - val_accuracy: 0.5203\n",
      "Epoch 33/150\n",
      "62/62 [==============================] - 1302s 21s/step - loss: 1.0583 - accuracy: 0.5285 - val_loss: 1.0770 - val_accuracy: 0.5264\n",
      "Epoch 34/150\n",
      "62/62 [==============================] - 1281s 21s/step - loss: 1.0232 - accuracy: 0.5391 - val_loss: 1.0706 - val_accuracy: 0.5366\n",
      "Epoch 35/150\n",
      "62/62 [==============================] - 1294s 21s/step - loss: 1.0272 - accuracy: 0.5417 - val_loss: 1.1045 - val_accuracy: 0.5285\n",
      "Epoch 36/150\n",
      "62/62 [==============================] - 1288s 21s/step - loss: 1.0245 - accuracy: 0.5401 - val_loss: 1.0892 - val_accuracy: 0.5366\n",
      "Epoch 37/150\n",
      "62/62 [==============================] - 1278s 21s/step - loss: 1.0344 - accuracy: 0.5437 - val_loss: 1.1197 - val_accuracy: 0.5020\n",
      "Epoch 38/150\n",
      "62/62 [==============================] - 1300s 21s/step - loss: 1.0224 - accuracy: 0.5478 - val_loss: 1.0619 - val_accuracy: 0.5183\n",
      "Epoch 39/150\n",
      "62/62 [==============================] - 1315s 21s/step - loss: 1.0205 - accuracy: 0.5534 - val_loss: 1.0785 - val_accuracy: 0.5488\n",
      "Epoch 40/150\n",
      "62/62 [==============================] - 1333s 22s/step - loss: 1.0004 - accuracy: 0.5544 - val_loss: 1.0810 - val_accuracy: 0.5224\n",
      "Epoch 41/150\n",
      "62/62 [==============================] - 1475s 24s/step - loss: 1.0075 - accuracy: 0.5437 - val_loss: 1.0603 - val_accuracy: 0.5244\n",
      "Epoch 42/150\n",
      "62/62 [==============================] - 1610s 26s/step - loss: 1.0079 - accuracy: 0.5564 - val_loss: 1.1191 - val_accuracy: 0.4959\n",
      "Epoch 43/150\n",
      "62/62 [==============================] - 1582s 26s/step - loss: 0.9955 - accuracy: 0.5518 - val_loss: 1.1429 - val_accuracy: 0.4898\n",
      "Epoch 44/150\n",
      "62/62 [==============================] - 1600s 26s/step - loss: 1.0023 - accuracy: 0.5650 - val_loss: 1.0631 - val_accuracy: 0.5407\n",
      "Epoch 45/150\n",
      "62/62 [==============================] - 1507s 24s/step - loss: 1.0036 - accuracy: 0.5564 - val_loss: 1.1328 - val_accuracy: 0.4939\n",
      "Epoch 46/150\n",
      "62/62 [==============================] - 1515s 24s/step - loss: 0.9892 - accuracy: 0.5595 - val_loss: 1.0787 - val_accuracy: 0.5183\n",
      "Epoch 47/150\n",
      "62/62 [==============================] - 1514s 24s/step - loss: 0.9848 - accuracy: 0.5747 - val_loss: 1.0589 - val_accuracy: 0.5549\n",
      "Epoch 48/150\n",
      "62/62 [==============================] - 5190s 85s/step - loss: 0.9767 - accuracy: 0.5615 - val_loss: 1.0709 - val_accuracy: 0.5264\n",
      "Epoch 49/150\n",
      "62/62 [==============================] - 1167s 19s/step - loss: 0.9823 - accuracy: 0.5671 - val_loss: 1.0920 - val_accuracy: 0.5183\n",
      "Epoch 50/150\n",
      "62/62 [==============================] - 1271s 21s/step - loss: 0.9596 - accuracy: 0.5838 - val_loss: 1.1110 - val_accuracy: 0.5122\n",
      "Epoch 51/150\n",
      "62/62 [==============================] - 1367s 22s/step - loss: 0.9753 - accuracy: 0.5676 - val_loss: 1.0641 - val_accuracy: 0.5569\n",
      "Epoch 52/150\n",
      "62/62 [==============================] - 1287s 21s/step - loss: 0.9866 - accuracy: 0.5681 - val_loss: 1.0584 - val_accuracy: 0.5467\n",
      "Epoch 53/150\n",
      "62/62 [==============================] - 1397s 23s/step - loss: 0.9757 - accuracy: 0.5722 - val_loss: 1.0758 - val_accuracy: 0.5447\n",
      "Epoch 54/150\n",
      "62/62 [==============================] - 1313s 21s/step - loss: 0.9554 - accuracy: 0.5838 - val_loss: 1.0802 - val_accuracy: 0.5427\n",
      "Epoch 55/150\n",
      "62/62 [==============================] - 1160s 19s/step - loss: 0.9507 - accuracy: 0.5889 - val_loss: 1.1022 - val_accuracy: 0.5122\n",
      "Epoch 56/150\n",
      "62/62 [==============================] - 1028s 17s/step - loss: 0.9555 - accuracy: 0.5716 - val_loss: 1.1322 - val_accuracy: 0.4858\n",
      "Epoch 57/150\n",
      "62/62 [==============================] - 1027s 17s/step - loss: 0.9576 - accuracy: 0.5915 - val_loss: 1.0773 - val_accuracy: 0.5285\n",
      "Epoch 58/150\n",
      "62/62 [==============================] - 1116s 18s/step - loss: 0.9530 - accuracy: 0.5849 - val_loss: 1.0727 - val_accuracy: 0.5589\n",
      "Epoch 59/150\n",
      "62/62 [==============================] - 1130s 18s/step - loss: 0.9598 - accuracy: 0.5833 - val_loss: 1.0963 - val_accuracy: 0.5142\n",
      "Epoch 60/150\n",
      "62/62 [==============================] - 1253s 20s/step - loss: 0.9416 - accuracy: 0.6001 - val_loss: 1.0846 - val_accuracy: 0.5407\n",
      "Epoch 61/150\n",
      "62/62 [==============================] - 1152s 19s/step - loss: 0.9378 - accuracy: 0.5955 - val_loss: 1.0853 - val_accuracy: 0.5346\n",
      "Epoch 62/150\n",
      "62/62 [==============================] - 1029s 17s/step - loss: 0.9430 - accuracy: 0.5981 - val_loss: 1.0979 - val_accuracy: 0.5346\n",
      "Epoch 63/150\n",
      "62/62 [==============================] - 1026s 17s/step - loss: 0.9303 - accuracy: 0.6021 - val_loss: 1.0787 - val_accuracy: 0.5386\n",
      "Epoch 64/150\n",
      "62/62 [==============================] - 1276s 21s/step - loss: 0.9319 - accuracy: 0.6138 - val_loss: 1.1324 - val_accuracy: 0.5041\n",
      "Epoch 65/150\n",
      "62/62 [==============================] - 1564s 25s/step - loss: 0.9153 - accuracy: 0.6199 - val_loss: 1.0711 - val_accuracy: 0.5407\n",
      "Epoch 66/150\n",
      "62/62 [==============================] - 1435s 23s/step - loss: 0.9164 - accuracy: 0.5986 - val_loss: 1.0835 - val_accuracy: 0.5163\n",
      "Epoch 67/150\n",
      "62/62 [==============================] - 1214s 20s/step - loss: 0.9183 - accuracy: 0.6057 - val_loss: 1.0664 - val_accuracy: 0.5630\n",
      "Epoch 68/150\n",
      "62/62 [==============================] - 1193s 19s/step - loss: 0.9038 - accuracy: 0.6148 - val_loss: 1.0698 - val_accuracy: 0.5305\n",
      "Epoch 69/150\n",
      "62/62 [==============================] - 1247s 20s/step - loss: 0.9030 - accuracy: 0.6184 - val_loss: 1.0695 - val_accuracy: 0.5569\n",
      "Epoch 70/150\n",
      "62/62 [==============================] - 3081s 50s/step - loss: 0.9199 - accuracy: 0.6026 - val_loss: 1.0825 - val_accuracy: 0.5244\n",
      "Epoch 71/150\n",
      "62/62 [==============================] - 1474s 24s/step - loss: 0.8876 - accuracy: 0.6225 - val_loss: 1.0609 - val_accuracy: 0.5711\n",
      "Epoch 72/150\n",
      "62/62 [==============================] - 1566s 25s/step - loss: 0.8996 - accuracy: 0.6357 - val_loss: 1.1163 - val_accuracy: 0.5000\n",
      "16/16 [==============================] - 74s 5s/step - loss: 1.0584 - accuracy: 0.5467\n",
      "Test Accuracy: 54.67%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "df['Combined_Content'] = df['Title'] + ' ' + df['Article Content']\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.dropna(subset=['Combined_Content', 'Label'])\n",
    "\n",
    "X = df['Combined_Content']\n",
    "y = df['Label']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
    "\n",
    "# Tokenize the text data\n",
    "max_words = 5000\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_length = 100\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 200, input_length=max_length))\n",
    "model.add(LSTM(1028, dropout=0.4, recurrent_dropout=0.4, return_sequences=True))\n",
    "model.add(LSTM(512, dropout=0.4, recurrent_dropout=0.4, return_sequences=True))  # Added another LSTM layer\n",
    "model.add(LSTM(256, dropout=0.4, recurrent_dropout=0.4))  # Added another LSTM layer\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.00001)   # Adjusted learning rate\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)  # Increased patience\n",
    "model.fit(X_train_padded, y_train, validation_data=(X_test_padded, y_test), epochs=150, batch_size=32, callbacks=[early_stopping])  \n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
